{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#imports block\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Models\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, log_loss, accuracy_score\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "#nltk.download()\n",
    "from nltk import word_tokenize, pos_tag\n",
    "#importing the Stemming function from nltk library\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Input\n",
    "fake_df = pd.read_csv('Fake.csv')\n",
    "true_df = pd.read_csv('True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   \n",
       "2  December 31, 2017   \n",
       "3  December 30, 2017   \n",
       "4  December 29, 2017   "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>31-Dec-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>31-Dec-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>30-Dec-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>29-Dec-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>25-Dec-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject       date  \n",
       "0  Donald Trump just couldn t wish all Americans ...    News  31-Dec-17  \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News  31-Dec-17  \n",
       "2  On Friday, it was revealed that former Milwauk...    News  30-Dec-17  \n",
       "3  On Christmas day, Donald Trump announced that ...    News  29-Dec-17  \n",
       "4  Pope Francis used his annual Christmas Day mes...    News  25-Dec-17  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WASHINGTON (Reuters) - The U.S. Senate will vo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump s transition is humming right along now,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spewing falsehoods has become the favorite pas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I feel like I ve been drugged by Obama s rhet...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ryan has never made any secret about his desir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44935</th>\n",
       "      <td>STRASBOURG (Reuters) - European Commission chi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44936</th>\n",
       "      <td>Just as they did during and in the aftermath o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44937</th>\n",
       "      <td>An Asian student reporter on assignment for ES...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44938</th>\n",
       "      <td>Why should armed civilians have to protect and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44939</th>\n",
       "      <td>WASHINGTON (Reuters) - Former Fox News anchor ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44940 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  class\n",
       "0      WASHINGTON (Reuters) - The U.S. Senate will vo...      1\n",
       "1      Trump s transition is humming right along now,...      0\n",
       "2      Spewing falsehoods has become the favorite pas...      0\n",
       "3       I feel like I ve been drugged by Obama s rhet...      0\n",
       "4      Ryan has never made any secret about his desir...      0\n",
       "...                                                  ...    ...\n",
       "44935  STRASBOURG (Reuters) - European Commission chi...      1\n",
       "44936  Just as they did during and in the aftermath o...      0\n",
       "44937  An Asian student reporter on assignment for ES...      0\n",
       "44938  Why should armed civilians have to protect and...      0\n",
       "44939  WASHINGTON (Reuters) - Former Fox News anchor ...      1\n",
       "\n",
       "[44940 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df['class'] = 0\n",
    "true_df['class'] = 1\n",
    "#Concat Full Data\n",
    "df = pd.concat([fake_df, true_df])\n",
    "#Shuffle the Data\n",
    "df = df.sample(frac = 1)\n",
    "df.reset_index(inplace = True)\n",
    "#Filter Required columns\n",
    "df= df[['text','class']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "#Text Processing pipeline: Lower case + remove punctuation + Tokenization + Lemmatization + Stop words Removal\n",
    "#Convert these to Vectors using BOW or TF-IDF or Embeddings(Glove, fasttext, Word2Vec)\n",
    "#Make Baseline ML models(Log Reg, SVM, Naive Bayes, Decision Trees, RF, XGBoost Classifier)\n",
    "#Hyperparameter Tuning of above models\n",
    "#Make DL models using CNN, LSTM's, BERT/RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisation Required??? \n",
    "\n",
    "Normalisation performs:\n",
    "\n",
    "    Converting dates to text\n",
    "    Numbers to text\n",
    "    Currency/Percent signs to text\n",
    "    Expanding of abbreviations (content dependent) NLP - Natural Language Processing, Neuro-linguistic programming, Non-Linear programming\n",
    "    Spelling mistakes correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from normalise.normalisation import normalise\n",
    "\n",
    "#def normalization(tokens):\n",
    "#    user_abbr = { 'NLP' : 'Natural Language Processing'}\n",
    "#    normalised_tokens = normalise(tokens, user_abbrevs = user_abbr, verbose = False)\n",
    "#    return normalised_tokens\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "def stemming(words):\n",
    "    porter = PorterStemmer()\n",
    "    stemmed_words = [porter.stem(word) for word in words]\n",
    "    return stemmed_words\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text_processed = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_processed\n",
    "\n",
    "\n",
    "#Lower text: text = text.lower()\n",
    "#Tokenization Code:  words = word_tokenize(text)\n",
    "#Parts Of Speech Tagging: words = pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donald trump wish americans happy new year leave instead give shout enemy hater dishonest fake news medium former reality show star one job country rapidly grow strong smart want wish friend supporter enemy hater even dishonest fake news medium happy healthy new year president angry pant tweet great year america country rapidly grow strong smart want wish friend supporter enemy hater even dishonest fake news medium happy healthy new year great year america donald trump realdonaldtrump december trump tweet go welll expect kind president send new year greeting like despicable petty infantile gibberish trump lack decency win even allow rise gutter long enough wish american citizen happy new year bishop talbert swan talbertswan december one like calvin calvinstowell december impeachment would make great year america also accept regain control congress miranda yaver mirandayaver december hear talk include many people hate wonder hate alan sandoval alansandoval december use word hater new year wish marlene marlene december say happy new year koren pollitt korencarpenter december trump new year eve tweet happy new year include many enemy fight lose badly know love donald trump realdonaldtrump december nothing new trump year trump direct message enemy hater new year easter thanksgiving anniversary pic twitter com fpae kypa daniel dale ddale december trump holiday tweet clearly presidential long work hallmark become president steven goodine sgoodine december always like difference last year filter break roy schulze thbthttt december apart teenager use term hater wendy wendywhistle december fucking year old know rainyday december people vote hole thinking would change get power wrong year old man change year old photo andrew burton getty image\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f795dd90ed445dad2780c8804c2d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>washington reuters senate vote wednesday wheth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trump transition hum right along complete ever...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spew falsehood become favorite pastime democra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feel like drug obama rhetoric greg gutfeldyes ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ryan never make secret desire welcome immigran...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44935</th>\n",
       "      <td>strasbourg reuters european commission chief j...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44936</th>\n",
       "      <td>aftermath hurricane katrina dozen year ago mex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44937</th>\n",
       "      <td>asian student reporter assignment espn cameram...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44938</th>\n",
       "      <td>armed civilian protect defend us military recr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44939</th>\n",
       "      <td>washington reuters former fox news anchor corr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44940 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  class\n",
       "0      washington reuters senate vote wednesday wheth...      1\n",
       "1      trump transition hum right along complete ever...      0\n",
       "2      spew falsehood become favorite pastime democra...      0\n",
       "3      feel like drug obama rhetoric greg gutfeldyes ...      0\n",
       "4      ryan never make secret desire welcome immigran...      0\n",
       "...                                                  ...    ...\n",
       "44935  strasbourg reuters european commission chief j...      1\n",
       "44936  aftermath hurricane katrina dozen year ago mex...      0\n",
       "44937  asian student reporter assignment espn cameram...      0\n",
       "44938  armed civilian protect defend us military recr...      0\n",
       "44939  washington reuters former fox news anchor corr...      1\n",
       "\n",
       "[44940 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\",'no','nor','not'])\n",
    "#Text preprocessing\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    #lower case text\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    #remove http links\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    #expanding some words\n",
    "    text = decontracted(text)\n",
    "    #remove special character\n",
    "    #text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "    #remove numbers\n",
    "    #text = re.sub(\"\\S*\\d\\S*\", \"\", text).strip()\n",
    "    \n",
    "    #text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def spacy_process(text):\n",
    "    #Cleaning the text\n",
    "    text = clean_text(text)\n",
    "    doc = nlp(text)\n",
    "    #print(text)\n",
    "    #Tokenization and lemmatization are done with the spacy nlp pipeline commands\n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        if len(token) > 1:\n",
    "            lemma_list.append(token.lemma_.lower())\n",
    "    #print(\"Tokenize+Lemmatize:\")\n",
    "    #print(lemma_list)\n",
    "    \n",
    "    #Filter the stopword\n",
    "    filtered_sentence =[] \n",
    "    for word in lemma_list:\n",
    "        if word not in custom_stopwords:\n",
    "            filtered_sentence.append(word)\n",
    "    \n",
    "    #Remove punctuation\n",
    "    punctuations=\"?:!.,;\"\n",
    "    for word in filtered_sentence:\n",
    "        if word in punctuations:\n",
    "            filtered_sentence.remove(word)\n",
    "\n",
    "    updated_text = \" \".join(filtered_sentence)\n",
    "    return updated_text\n",
    "\n",
    "first_row = fake_df.iloc[0]\n",
    "print(spacy_process(first_row['text']))\n",
    "\n",
    "df['text'] = df['text'].progress_apply(lambda x: spacy_process(x))\n",
    "df.to_excel(\"processed_news.xlsx\", index = False)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Vectorization, Hyperparamter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Bulding:\n",
    "\n",
    "used 5 fold cross validation for the below models. Generally Naive Bayes is used for baselining.\n",
    "\n",
    "Naive Bayes\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "Support Vector Classifier\n",
    "\n",
    "Decision Tree\n",
    "\n",
    "K Nearest Neighbor\n",
    "\n",
    "Random Forest\n",
    "\n",
    "Extreme Gradient Boosting\n",
    "\n",
    "Soft Voting Classifier - All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (2400,)\n",
      "y_train shape (2400,)\n",
      "X_test shape (600,)\n",
      "y_test shape (600,)\n",
      "\n",
      "ratio of target in y_train : [0.5225 0.4775]\n",
      "ratio of target in y_test : [0.52333333 0.47666667]\n",
      "ratio of target in original_data : [0.52266667 0.47733333]\n"
     ]
    }
   ],
   "source": [
    "#Train Test Split\n",
    "# We will use StratifiedShuffleSplit to split the data Taking into consideration that we will get the same ratio on the target column\n",
    "\n",
    "df = pd.read_excel(\"processed_news.xlsx\")\n",
    "df = df[df['text'].notna()]\n",
    "\n",
    "df = df.iloc[:3000]\n",
    "X = df['text']\n",
    "y = df['class']\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train, test in sss.split(X, y):\n",
    "    X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "    y_train, y_test = y.iloc[train], y.iloc[test]\n",
    "    \n",
    "print('X_train shape', X_train.shape)\n",
    "print('y_train shape', y_train.shape)\n",
    "print('X_test shape', X_test.shape)\n",
    "print('y_test shape', y_test.shape)\n",
    "\n",
    "# almost same ratio\n",
    "print('\\nratio of target in y_train :',y_train.value_counts().values/ len(y_train))\n",
    "print('ratio of target in y_test :',y_test.value_counts().values/ len(y_test))\n",
    "print('ratio of target in original_data :',df['class'].value_counts().values/ len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vectorization = TfidfVectorizer()\n",
    "X_train_vectors = vectorization.fit_transform(X_train)\n",
    "X_test_vectors = vectorization.transform(X_test)\n",
    "\n",
    "X_train_vectors = X_train_vectors.todense()\n",
    "X_test_vectors = X_test_vectors.todense()\n",
    "\n",
    "X_train_vectors = np.array(X_train_vectors)\n",
    "X_test_vectors = np.array(X_test_vectors)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loss(y_true, y_pred, return_flag=False):\n",
    "    pre = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    loss = log_loss(y_true, y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if return_flag:\n",
    "        return pre, rec, f1, loss, acc\n",
    "    else:\n",
    "        print('  pre: %.3f\\n  rec: %.3f\\n  f1: %.3f\\n  loss: %.3f\\n  acc: %.3f' % (pre, rec, f1, loss, acc))\n",
    "\n",
    "\n",
    "def simple_models(models, X_train, y_train):\n",
    "    for name, model in models.items():\n",
    "        cv = cross_val_score(model,X_train, y_train, cv =5)\n",
    "        print(\"*********************** \"+name+\" *************************\")\n",
    "        print(cv)\n",
    "        print(cv.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************** GaussianNaiveBayes *************************\n",
      "[0.7875     0.84583333 0.82083333 0.775      0.75833333]\n",
      "0.7975\n",
      "*********************** LogisticRegression *************************\n",
      "[0.97083333 0.95625    0.94791667 0.95833333 0.95625   ]\n",
      "0.9579166666666667\n",
      "*********************** DecisionTreeClassifier *************************\n",
      "[0.99583333 0.98541667 0.97708333 0.99375    0.98541667]\n",
      "0.9875\n",
      "*********************** KNeighborsClassifier *************************\n",
      "[0.82291667 0.83541667 0.8375     0.8125     0.82083333]\n",
      "0.8258333333333333\n",
      "*********************** RandomForestClassifier *************************\n",
      "[0.97291667 0.97708333 0.96666667 0.9625     0.97708333]\n",
      "0.9712499999999998\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'GaussianNaiveBayes': GaussianNB(),\n",
    "    'LogisticRegression': LogisticRegression(max_iter = 2000,random_state=42),\n",
    "    #'SVC': SVC(random_state=42),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'RandomForestClassifier':RandomForestClassifier(random_state = 42)\n",
    "}\n",
    "#Add XGBoost, voting classifiers\n",
    "#voting_clf = VotingClassifier(estimators = [('lr',lr_model),('rf',rf_model),('xgb',xgb_model)], voting = 'soft') \n",
    "\n",
    "simple_models(models, X_train_vectors, y_train)\n",
    "#After checking Cross Validation scores then Fit the train data to a model and predict output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edaef9d03e0964944a4ac8a8046f0ca7775b12f92df181532561f3229a0e765e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
